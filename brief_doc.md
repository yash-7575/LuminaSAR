# ğŸ”† LuminaSAR â€” Complete Project Brief

> **SAR Narrative Generator with Explainable AI Audit Trail**
>
> Barclays Hack-O-Hire 2026

---

## Table of Contents

1. [What is a SAR & Why Automate It?](#1-what-is-a-sar--why-automate-it)
2. [Project Overview](#2-project-overview)
3. [High-Level Architecture Diagram](#3-high-level-architecture-diagram)
4. [Detailed Data Flow â€” End to End](#4-detailed-data-flow--end-to-end)
5. [Backend Deep Dive â€” FastAPI](#5-backend-deep-dive--fastapi)
6. [Database Schema & How It Was Built](#6-database-schema--how-it-was-built)
7. [The 6-Step AI Pipeline â€” LangGraph](#7-the-6-step-ai-pipeline--langgraph)
8. [How Pattern Detection (ML) Works](#8-how-pattern-detection-ml-works)
9. [How RAG Works â€” ChromaDB + Sentence-Transformers](#9-how-rag-works--chromadb--sentence-transformers)
10. [How Ollama Generates Text â€” LLM Integration](#10-how-ollama-generates-text--llm-integration)
11. [The Anti-Hallucination Validator](#11-the-anti-hallucination-validator)
12. [Hash-Chained Audit Trail â€” The Innovation](#12-hash-chained-audit-trail--the-innovation)
13. [Frontend â€” React + TypeScript](#13-frontend--react--typescript)
14. [How Frontend Fetches Data from Backend](#14-how-frontend-fetches-data-from-backend)
15. [Docker & Deployment](#15-docker--deployment)
16. [What I Built & In What Order](#16-what-i-built--in-what-order)
17. [Essential Topics to Learn](#17-essential-topics-to-learn)
18. [Similar Project Ideas](#18-similar-project-ideas)

---

## 1. What is a SAR & Why Automate It?

A **Suspicious Activity Report (SAR)** is a regulatory document that banks must file with the Financial Intelligence Unit (FIU) whenever they detect potentially fraudulent or money-laundering transactions. Banks like Barclays file **thousands of SARs per year**.

### The Problem

| Manual Process | LuminaSAR |
|---|---|
| 5-6 hours per SAR | ~30 seconds |
| Human error in data citation | 100% data-grounded |
| No audit trail of reasoning | Full hash-chained audit trail |
| Black-box decisions | Sentence-level explainability |

**Key Point**: A compliance analyst currently reads through hundreds of transactions, identifies suspicious patterns, writes a formal regulatory narrative, and cites specific data points. This is tedious, error-prone, and expensive. Our AI does it in 30 seconds with full transparency.

---

## 2. Project Overview

LuminaSAR is a **full-stack GenAI application** with three major layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       USER (Browser)                         â”‚
â”‚          Dashboard â†’ Generate SAR â†’ View Report              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   FRONTEND (React + TS)                      â”‚
â”‚   Vite â”‚ TailwindCSS â”‚ React Query â”‚ Zustand â”‚ Framer Motion â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     REST API (HTTPS)                         â”‚
â”‚      axios.post("http://localhost:8000/api/v1/sar/...")      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   BACKEND (FastAPI)                           â”‚
â”‚   Routes â†’ Schemas (Pydantic) â†’ Services â†’ Models (ORM)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      AI / ML Layer                           â”‚
â”‚   Pattern Detection â”‚ RAG â”‚ LLM (Ollama) â”‚ Validation        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      DATA LAYER                              â”‚
â”‚   Supabase PostgreSQL / SQLite â”‚ ChromaDB (Vector DB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. High-Level Architecture Diagram

```
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    User's       â”‚
                           â”‚    Browser      â”‚
                           â”‚  (React App)    â”‚
                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP Requests (axios)
                                  â”‚ GET/POST to localhost:8000
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        FastAPI Backend       â”‚
                    â”‚     (localhost:8000)          â”‚
                    â”‚                              â”‚
                    â”‚  GET /health                  â”‚
                    â”‚  POST /api/v1/sar/generate    â”‚
                    â”‚  GET  /api/v1/sar/{id}        â”‚
                    â”‚  GET  /api/v1/sar/{id}/audit  â”‚
                    â”‚  GET  /api/v1/sar/stats       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  SQLAlchemy â”‚ â”‚   LangGraph    â”‚
                    â”‚  ORM Layer  â”‚ â”‚   Workflow     â”‚
                    â”‚  (models/)  â”‚ â”‚  (6 steps)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”‚
                    â”‚  Database   â”‚       â”œâ”€â”€â†’ PatternDetector (ML)
                    â”‚  Supabase   â”‚       â”‚     â””â”€ pandas, numpy, networkx
                    â”‚  PostgreSQL â”‚       â”‚
                    â”‚     OR      â”‚       â”œâ”€â”€â†’ RAGService
                    â”‚  SQLite     â”‚       â”‚     â””â”€ ChromaDB + sentence-transformers
                    â”‚  (fallback) â”‚       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”œâ”€â”€â†’ LLMService
                                          â”‚     â””â”€ Ollama HTTP API (llama3.2)
                                          â”‚
                                          â”œâ”€â”€â†’ NarrativeValidator
                                          â”‚     â””â”€ Anti-hallucination checks
                                          â”‚
                                          â””â”€â”€â†’ AuditLogger
                                                â””â”€ SHA-256 hash chain
```

### Component Interaction Sequence

```
   Frontend                Backend                    AI Services
      â”‚                       â”‚                            â”‚
      â”‚  POST /generate       â”‚                            â”‚
      â”‚  {case_id}            â”‚                            â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚                            â”‚
      â”‚                       â”‚ 1. Fetch customer + txns   â”‚
      â”‚                       â”‚    from DB                 â”‚
      â”‚                       â”‚                            â”‚
      â”‚                       â”‚ 2. PatternDetector.analyze â”‚
      â”‚                       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      â”‚                       â”‚    velocity, volume,       â”‚
      â”‚                       â”‚    structuring, network    â”‚
      â”‚                       â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚                       â”‚                            â”‚
      â”‚                       â”‚ 3. RAGService.retrieve     â”‚
      â”‚                       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      â”‚                       â”‚    ChromaDB semantic search â”‚
      â”‚                       â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚                       â”‚                            â”‚
      â”‚                       â”‚ 4. LLMService.generate     â”‚
      â”‚                       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      â”‚                       â”‚    HTTP POST to Ollama      â”‚
      â”‚                       â”‚    localhost:11434           â”‚
      â”‚                       â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚                       â”‚                            â”‚
      â”‚                       â”‚ 5. Validate narrative      â”‚
      â”‚                       â”‚ 6. Save + audit trail      â”‚
      â”‚                       â”‚                            â”‚
      â”‚  {narrative, audit}   â”‚                            â”‚
      â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                            â”‚
      â”‚                       â”‚                            â”‚
```

---

## 4. Detailed Data Flow â€” End to End

Here's exactly what happens when a user clicks **"Generate SAR"**:

### ğŸ–±ï¸ Step 1: User Clicks "Generate" in the Browser

The React frontend (`GenerateSAR.tsx`) collects the `case_id` from the input field and calls:

```typescript
// frontend/src/services/api.ts
const response = await apiClient.post('/api/v1/sar/generate', {
    case_id: "b920a3c4-dccc-44ce-9ebb-9866edcec4dd"
})
```

This is an HTTP POST request from React (port 5173) â†’ FastAPI (port 8000).

### âš¡ Step 2: FastAPI Receives the Request

The backend route handler in `backend/app/routes/sar.py`:

```python
@router.post("/generate")
async def generate_sar(request: GenerateSARRequest, db: Session = Depends(get_db)):
    result = await run_sar_workflow(case.case_id, case.customer_id, db)
```

FastAPI:
- Validates the request with Pydantic (`GenerateSARRequest`)
- Injects a database session via `Depends(get_db)`
- Calls the LangGraph workflow

### ğŸ”„ Step 3: LangGraph Runs 6 Pipeline Steps

The `langgraph_workflow.py` orchestrates everything. Each step reads the `SARState` dict, does its work, and passes it to the next step. Every step logs to the `AuditLogger`.

### ğŸ“¤ Step 4: Response Goes Back to React

The backend returns a JSON response:

```json
{
    "narrative_id": "abc-123",
    "narrative_text": "SUSPICIOUS ACTIVITY REPORT...",
    "risk_score": 4.5,
    "typologies": ["structuring", "layering"],
    "generation_time_seconds": 34,
    "audit_steps": 6
}
```

React then navigates the user to the **SAREditor** page to view and interact with the narrative.

---

## 5. Backend Deep Dive â€” FastAPI

### Why FastAPI?

FastAPI was chosen because:

1. **Async support** â€” LLM calls take 20-30 seconds; async lets us handle multiple requests
2. **Pydantic integration** â€” Automatic request/response validation and OpenAPI docs
3. **Dependency Injection** â€” `Depends(get_db)` cleanly provides database sessions
4. **Auto-generated docs** â€” Swagger UI at `/docs` for free

### How the Backend is Organized

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py           â† FastAPI app creation + CORS + router includes
â”‚   â”œâ”€â”€ config.py          â† Pydantic Settings loading from .env
â”‚   â”œâ”€â”€ database.py        â† SQLAlchemy engine + session factory
â”‚   â”œâ”€â”€ models/            â† ORM models (database tables)
â”‚   â”œâ”€â”€ schemas/           â† Pydantic models (API request/response shapes)
â”‚   â”œâ”€â”€ routes/            â† API endpoint handlers
â”‚   â”œâ”€â”€ services/          â† Business logic (AI/ML services)
â”‚   â””â”€â”€ utils/             â† Helper functions (hash, prompts)
```

### Key Concepts Used

**1. CORS Middleware** (`main.py`):
```python
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"])
```
This allows the React frontend (different port 5173) to talk to the backend (port 8000). Without CORS, the browser blocks cross-origin requests.

**2. Dependency Injection** (`database.py`):
```python
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```
Every route that needs the database declares `db: Session = Depends(get_db)`. FastAPI automatically creates a session, gives it to the route, and closes it after the request.

**3. Pydantic Validation** (`schemas/request.py`):
```python
class GenerateSARRequest(BaseModel):
    case_id: str
    force_regenerate: bool = False
```
If anyone sends invalid data (e.g., missing `case_id`), FastAPI automatically returns a 422 error with detailed validation messages.

---

## 6. Database Schema & How It Was Built

### Schema Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    customers     â”‚     â”‚   transactions   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ customer_id (PK) â”‚â†â”€â”€â”€â”€â”‚ customer_id (FK) â”‚
â”‚ name             â”‚     â”‚ transaction_id   â”‚
â”‚ account_number   â”‚     â”‚ amount           â”‚
â”‚ occupation       â”‚     â”‚ date             â”‚
â”‚ stated_income    â”‚     â”‚ source_account   â”‚
â”‚ customer_since   â”‚     â”‚ dest_account     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ transaction_type â”‚
         â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    sar_cases     â”‚     â”‚  sar_narratives  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ case_id (PK)     â”‚â†â”€â”€â”€â”€â”‚ case_id (FK)     â”‚
â”‚ customer_id (FK) â”‚     â”‚ narrative_id (PK)â”‚â”€â”€â”
â”‚ status           â”‚     â”‚ narrative_text   â”‚  â”‚
â”‚ risk_score       â”‚     â”‚ generated_at     â”‚  â”‚
â”‚ typologies (JSON)â”‚     â”‚ gen_time_seconds â”‚  â”‚
â”‚ created_at       â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
                                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                         â”‚   audit_trail    â”‚   â”‚
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
                         â”‚ audit_id (PK)    â”‚   â”‚
                         â”‚ narrative_id (FK)â”‚â†â”€â”€â”˜
                         â”‚ step_name        â”‚
                         â”‚ data_sources JSONâ”‚
                         â”‚ reasoning JSON   â”‚
                         â”‚ confidence JSON  â”‚
                         â”‚ previous_hash    â”‚ â† SHA-256 chain
                         â”‚ current_hash     â”‚ â† SHA-256 chain
                         â”‚ logged_at        â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How the Tables Connect

1. A **Customer** has many **Transactions** (1-to-many via `customer_id`)
2. A **Customer** has many **SAR Cases** (when suspicious activity is flagged)
3. A **SAR Case** can have one **SAR Narrative** (the AI-generated report)
4. A **SAR Narrative** has many **Audit Trail** entries (one per pipeline step)

### How SQLAlchemy Models Work

Each Python class maps to a database table:

```python
# backend/app/models/customer.py
class Customer(Base):
    __tablename__ = "customers"
    customer_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    name = Column(String, nullable=True)
    account_number = Column(String, nullable=True)
    ...
```

When SQLAlchemy runs a query like `db.query(Customer).filter(...)`, it translates it to SQL:

```sql
SELECT * FROM customers WHERE customer_id = 'abc-123';
```

### Dual Database Strategy

The `database.py` file implements a smart fallback:

```
1. Try connecting to Supabase PostgreSQL (cloud database)
2. If that fails (DNS error, auth error, etc.)
   â†’ Automatically create a local SQLite file (luminasar.db)
   â†’ Create all tables in SQLite
   â†’ App runs perfectly on SQLite
```

This means the app works anywhere â€” with internet (Supabase) or without (SQLite).

---

## 7. The 6-Step AI Pipeline â€” LangGraph

The heart of LuminaSAR is the **6-step AI pipeline** in `langgraph_workflow.py`. It uses a TypedDict called `SARState` that flows through every step:

```python
class SARState(TypedDict):
    case_id: str
    customer_id: str
    customer_data: Dict        # Filled by Step 1
    transactions: List[Dict]   # Filled by Step 1
    patterns: Dict             # Filled by Step 2
    typologies: List[str]      # Filled by Step 2
    templates: List[str]       # Filled by Step 3
    narrative: str             # Filled by Step 4
    audit_logs: List[Dict]     # Appended by every step
    risk_score: float          # Filled by Step 2
    narrative_id: str          # Filled by Step 6
    error: Optional[str]       # Set if any step fails
```

### The Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: FETCH  â”‚  Query database for customer KYC + transaction records
â”‚  _fetch_data()  â”‚  â†’ Fills: customer_data, transactions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: ANALYZE   â”‚  Run 4 ML algorithms on transactions DataFrame
â”‚  _analyze_patterns â”‚  â†’ Fills: patterns, typologies, risk_score
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: RAG RETRIEVE  â”‚  Search ChromaDB for relevant SAR templates
â”‚  _retrieve_templates   â”‚  â†’ Fills: templates
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: GENERATE    â”‚  Send prompt to Ollama llama3.2 model
â”‚  _generate_narrative â”‚  â†’ Fills: narrative (400-600 words)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: VALIDATE    â”‚  Cross-check narrative vs source data
â”‚  _validate()         â”‚  â†’ Checks for hallucinated amounts/dates
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: SAVE        â”‚  Persist narrative + audit trail to database
â”‚  _save_results()     â”‚  â†’ Fills: narrative_id, commits to DB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why LangGraph?

LangGraph (from LangChain) provides:
- **Stateful workflows** â€” the `SARState` dict is passed between steps
- **Error propagation** â€” if any step sets `state["error"]`, the pipeline stops
- **Composability** â€” steps can be added, removed, or reordered easily

---

## 8. How Pattern Detection (ML) Works

The `PatternDetector` class in `pattern_detector.py` runs **4 independent ML/statistical algorithms** on a Pandas DataFrame of transactions:

### Algorithm 1: Velocity Analysis

**What it detects**: Money moving suspiciously fast (many transactions in few days).

```
Example:
  50 transactions in 3 days â†’ HIGH risk
  50 transactions in 60 days â†’ LOW risk
```

**How it works**:
```python
time_span = (max_date - min_date).days
transactions_per_day = len(df) / max(time_span, 1)
# < 7 days = HIGH, < 30 days = MEDIUM, else LOW
```

### Algorithm 2: Volume Analysis

**What it detects**: Unusually large total amounts being moved.

```python
total_amount = df["amount"].sum()
avg_amount = df["amount"].mean()
max_amount = df["amount"].max()
```

Total above â‚¹1 Crore = very suspicious. Average amount much higher than stated income = suspicious.

### Algorithm 3: Structuring Detection

**What it detects**: Transactions deliberately kept just below â‚¹50,000 (the CTR reporting threshold in India).

```
Example:
  â‚¹49,000, â‚¹48,500, â‚¹49,900 â†’ Structuring detected!
  (Someone is splitting a large transfer into amounts below â‚¹50K to avoid automatic reporting)
```

**How it works**:
```python
threshold = 50000  # INR CTR threshold
# Find transactions between 90-100% of threshold (â‚¹45,000 - â‚¹49,999)
near_threshold = amounts[(amounts >= threshold * 0.90) & (amounts < threshold)]
structuring_likelihood = len(near_threshold) / len(amounts)
```

If more than 30% of transactions are near-threshold â†’ structuring is flagged.

### Algorithm 4: Network Graph Analysis (NetworkX)

**What it detects**: Suspicious account-to-account relationships.

```
Example: If 25 different source accounts all send money to the SAME
destination account â†’ "Smurfing" (many small deposits to evade detection)
```

**How it works**:
1. Build a **directed graph** where nodes = account numbers and edges = transactions
2. Calculate **degree centrality** â€” accounts that connect to many others are "hubs"
3. Detect **fan-in** (many sources â†’ 1 destination) and **fan-out** (1 source â†’ many destinations)

```python
import networkx as nx
G = nx.DiGraph()
G.add_edge(source_account, dest_account, amount=amt)
centrality = nx.degree_centrality(G)
```

### Typology Matching

After all 4 algorithms run, the results are matched against 6 money laundering typologies:

| Typology | Detection Rule |
|---|---|
| **Layering** | Rapid movement (< 7 days) + many distinct sources |
| **Structuring** | > 30% of transactions near â‚¹50K threshold |
| **Smurfing** | > 15 unique source accounts feeding one destination |
| **Integration** | > â‚¹50 Lakh in < 14 days |
| **Round-tripping** | High fan-in AND high fan-out |
| **Funnel Account** | High hub centrality in network graph |

### Risk Score Calculation

The risk score (0-10) is a weighted sum:
- Velocity: 0-30 points
- Volume: 0-25 points
- Structuring: 0-25 points
- Network: 0-20 points
- Total `/10` and capped at 10.0

---

## 9. How RAG Works â€” ChromaDB + Sentence-Transformers

### What is RAG?

**RAG = Retrieval-Augmented Generation**. Instead of asking the LLM to generate text from scratch, we first *retrieve* relevant reference documents and include them in the prompt. This:

1. **Improves quality** â€” The LLM has examples to follow
2. **Reduces hallucination** â€” The LLM is grounded in real templates
3. **Enables specialization** â€” Different templates for different crime types

### How Our RAG Pipeline Works

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   SAR Template Files (.txt)   â”‚
                        â”‚   sar_structuring.txt         â”‚
                        â”‚   sar_layering.txt            â”‚
                        â”‚   sar_smurfing.txt            â”‚
                        â”‚   sar_integration.txt         â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚  Load templates
                                     â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Sentence-Transformers      â”‚
                        â”‚   Model: all-MiniLM-L6-v2    â”‚
                        â”‚   Converts text â†’ 384-dim    â”‚
                        â”‚   vector embedding           â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚  Store embeddings
                                     â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚        ChromaDB              â”‚
                        â”‚   Collection: sar_templates  â”‚
                        â”‚   Stores: text + embedding   â”‚
                        â”‚   + metadata (typology, src) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   AT QUERY TIME:          â”‚
                         â”‚   1. Detected typologies  â”‚
                         â”‚      ["structuring"]      â”‚
                         â”‚   2. Create query text    â”‚
                         â”‚   3. Encode to embedding  â”‚
                         â”‚   4. Similarity search    â”‚
                         â”‚   5. Return top-k matches â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step-by-Step RAG Flow

**Step A: Loading Templates (Happens on Startup)**

```python
# rag_service.py â†’ load_templates()
1. Read all .txt files from backend/data/templates/
2. For each file:
   a. Read the content
   b. Extract the typology from the filename (e.g., "sar_layering.txt" â†’ "layering")
   c. Encode the text to a 384-dimensional vector using sentence-transformers
   d. Store in ChromaDB: {document: text, embedding: vector, metadata: {typology, source}}
```

**Step B: Retrieving Templates (During SAR Generation)**

```python
# rag_service.py â†’ retrieve_templates(typologies=["structuring", "layering"])
1. Build a query string: "SAR narrative for structuring, layering money laundering"
2. Encode the query to a 384-dimensional vector
3. ChromaDB performs cosine similarity search against all stored template embeddings
4. Return top 3 most similar templates
```

### What are Embeddings (Simplified)?

An embedding is a list of numbers that represents the "meaning" of text:

```
"money laundering structuring" â†’ [0.12, -0.45, 0.78, ..., 0.33]  (384 numbers)
"transactions below threshold" â†’ [0.15, -0.42, 0.75, ..., 0.30]  (similar numbers!)
"cat sitting on mat"           â†’ [-0.8, 0.61, -0.22, ..., 0.99]  (very different!)
```

ChromaDB uses **cosine similarity** to find which stored templates are most similar to the query. Templates about structuring will rank highest when the detected typology is structuring.

---

## 10. How Ollama Generates Text â€” LLM Integration

### What is Ollama?

Ollama is a **local LLM runtime** â€” it runs AI language models (like llama3.2) entirely on your computer. No API keys, no cloud, no data leakage.

### How We Connect to Ollama

The connection is simple â€” Ollama exposes a **REST API** on `http://localhost:11434`:

```python
# llm_service.py â†’ generate_narrative()

async with httpx.AsyncClient(timeout=120.0) as client:
    response = await client.post(
        "http://localhost:11434/api/generate",  # Ollama's API endpoint
        json={
            "model": "llama3.2:latest",
            "prompt": prompt,                    # Our constructed prompt
            "stream": False,                     # Wait for complete response
            "options": {
                "temperature": 0.3,              # Low = more factual, less creative
                "num_predict": 2000,             # Max output tokens
                "top_p": 0.9,                    # Nucleus sampling
            },
        },
    )
```

### The Prompt Engineering

The magic is in **how we construct the prompt** (`utils/prompts.py`). The prompt includes:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SYSTEM INSTRUCTION                        â•‘
â•‘  "You are a senior bank compliance analyst â•‘
â•‘   writing a SAR for FIU-IND..."            â•‘
â•‘                                            â•‘
â•‘  CRITICAL: Use ONLY the data below.        â•‘
â•‘  DO NOT invent amounts or dates.           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CUSTOMER DATA (from database)             â•‘
â•‘  Name: Rajesh Malhotra                     â•‘
â•‘  Account: HDFC203847591                    â•‘
â•‘  Occupation: Import/Export Business         â•‘
â•‘  Income: â‚¹12,00,000                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TRANSACTIONS (from database)              â•‘
â•‘  - â‚¹49,000 on 2026-01-15 from HDFC â†’ BOB  â•‘
â•‘  - â‚¹48,500 on 2026-01-16 from HDFC â†’ SBI  â•‘
â•‘  ... (up to 25 transactions)               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  DETECTED PATTERNS (from PatternDetector)  â•‘
â•‘  Risk Score: 6.5/10                        â•‘
â•‘  Typologies: structuring, layering         â•‘
â•‘  Velocity: 14 days, 3.5 txn/day           â•‘
â•‘  Structuring Likelihood: 45.0%             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  REFERENCE TEMPLATES (from RAG/ChromaDB)   â•‘
â•‘  [Retrieved SAR template about structuring] â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  YOUR TASK:                                â•‘
â•‘  Write a 400-600 word SAR narrative with:  â•‘
â•‘  1. Subject Information                    â•‘
â•‘  2. Suspicious Activity Description        â•‘
â•‘  3. Supporting Evidence                    â•‘
â•‘  4. Analyst Assessment                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Why Temperature 0.3?

| Temperature | Effect |
|---|---|
| 0.0 | Completely deterministic, always picks most likely token |
| 0.3 | Mostly factual, slight variation in wording â† **We use this** |
| 0.7 | Balanced creativity and accuracy |
| 1.0 | Very creative, more hallucination risk |

For compliance documents, we want **factual accuracy over creativity**, hence 0.3.

---

## 11. The Anti-Hallucination Validator

After the LLM generates text, we run **two validation passes** before saving:

### Pass 1: Structural Validator (`validator.py`)

```python
class NarrativeValidator:
    def validate(self, narrative, transactions, customer):
        # 1. Is the customer's name mentioned?
        # 2. Is the account number referenced?
        # 3. Is the narrative at least 100 words?
        # 4. Does it contain generic AI responses ("As an AI...")?
        # 5. Does it have SAR structure keywords ("suspicious", "transaction")?
```

### Pass 2: Amount Cross-Reference (`llm_service.py`)

```python
def validate_narrative(self, narrative, source_data):
    # 1. Extract all â‚¹ amounts from the narrative using regex
    # 2. Extract all transaction amounts from source data
    # 3. Check if every amount in the narrative exists in source data
    # 4. Flag any amount > â‚¹1,000 that doesn't match source data
```

This catches hallucination like:
- âŒ LLM says "â‚¹5,75,000 was transferred" but no such amount exists in data
- âœ… LLM says "â‚¹49,000 was transferred" and â‚¹49,000 exists in transactions

---

## 12. Hash-Chained Audit Trail â€” The Innovation

This is the **key differentiator** of LuminaSAR â€” every step of the AI pipeline is logged with a **SHA-256 hash chain**, similar to how blockchain works.

### How the Hash Chain Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: fetch_data                                       â”‚
â”‚  data_sources: {customer_id: "abc", database: "supabase"} â”‚
â”‚  reasoning: {action: "Fetched KYC data", customer: "..."}â”‚
â”‚  confidence: 1.0                                          â”‚
â”‚  previous_hash: "0000...0000" (genesis hash)              â”‚
â”‚  current_hash: SHA256(all above fields) = "a3f7..."       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 2: analyze_patterns                                 â”‚
â”‚  data_sources: {transaction_ids, algorithms}               â”‚
â”‚  reasoning: {velocity, structuring, network}               â”‚
â”‚  confidence: 0.92                                          â”‚
â”‚  previous_hash: "a3f7..." â† links to Step 1!             â”‚
â”‚  current_hash: SHA256(all above) = "e8b2..."              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 3: retrieve_templates                               â”‚
â”‚  previous_hash: "e8b2..." â† links to Step 2!             â”‚
â”‚  current_hash: "c4d1..."                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ... and so on for all 6 steps                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Hash Chains?

Each step's `current_hash` is computed from ALL its data + the `previous_hash`. This means:

1. **Tamper-evident** â€” If anyone modifies a step's data, its hash changes, breaking the chain
2. **Ordered** â€” You can verify the exact sequence of AI decisions
3. **Regulatory-compliant** â€” Auditors can independently verify that no step was altered

### Verification

```python
def verify_chain(self) -> bool:
    for i in range(1, len(self.logs)):
        # Check chain link
        if self.logs[i]["previous_hash"] != self.logs[i-1]["current_hash"]:
            return False  # Chain is broken!
        # Recompute hash to verify integrity
        expected = compute_hash(self.logs[i], exclude_keys=["current_hash"])
        if self.logs[i]["current_hash"] != expected:
            return False  # Data was tampered!
    return True
```

### Sentence-Level Attribution

The `create_sentence_attribution()` method maps **each sentence** in the narrative back to specific transactions:

```python
# For each sentence in the narrative:
#   - Check if any transaction ID appears in it
#   - Check if any transaction amount appears in it
#   - Check if any account number appears in it
#   - Record which data points were referenced
```

This powers the **click-to-audit** feature in the frontend â€” analysts click a sentence and see exactly which transactions informed it.

---

## 13. Frontend â€” React + TypeScript

### Tech Stack

| Library | Purpose |
|---|---|
| **React 18** | UI component library |
| **TypeScript** | Type-safe JavaScript |
| **Vite** | Fast build tool (replaces Webpack) |
| **TailwindCSS** | Utility-first CSS framework |
| **React Query** (@tanstack/react-query) | Server state management (API caching) |
| **Zustand** | Client state management |
| **Framer Motion** | Smooth animations |
| **Lucide React** | Icon library |
| **React Router DOM** | Client-side routing |

### Page Structure

```
App.tsx  (Router)
  â”œâ”€â”€ /          â†’ Dashboard.tsx    (Stats, recent cases, "New SAR" button)
  â”œâ”€â”€ /generate  â†’ GenerateSAR.tsx  (Case ID input, 6-step progress animation)
  â””â”€â”€ /editor/:id â†’ SAREditor.tsx   (Narrative viewer + audit trail panel)
```

### Dashboard (`Dashboard.tsx`)

- Fetches statistics from `GET /api/v1/sar/stats/overview`
- Shows 6 glassmorphism stat cards (total SARs, pending cases, avg time, etc.)
- Fetches recent cases from `GET /api/v1/sar/`
- Displays a table with case status, risk scores, and typology badges

### Generate SAR (`GenerateSAR.tsx`)

- Input field for Case ID
- On submit: calls `POST /api/v1/sar/generate`
- Shows a **6-step progress animation** while waiting (fetch â†’ analyze â†’ retrieve â†’ generate â†’ validate â†’ save)
- On completion: navigates to `/editor/{narrative_id}`

### SAR Editor (`SAREditor.tsx`)

- Fetches narrative from `GET /api/v1/sar/{id}`
- Fetches audit trail from `GET /api/v1/sar/{id}/audit`
- Displays the narrative with **clickable sentences**
- On sentence click: opens an audit panel showing data sources and reasoning
- Approve button calls `POST /api/v1/sar/{id}/approve`

---

## 14. How Frontend Fetches Data from Backend

### The API Client

All backend calls go through a centralized Axios client (`frontend/src/services/api.ts`):

```typescript
const apiClient = axios.create({
    baseURL: 'http://localhost:8000',
    timeout: 180000,  // 3 minutes (for LLM generation)
    headers: { 'Content-Type': 'application/json' },
})
```

### TypeScript Interfaces

We define the exact shape of API responses:

```typescript
export interface DashboardStats {
    total_sars: number
    pending_cases: number
    avg_generation_time: number
    ...
}
```

This means TypeScript will catch errors at compile time if the backend response doesn't match.

### API Methods

```typescript
export const api = {
    health:        () => apiClient.get('/health'),
    generateSAR:   (data) => apiClient.post('/api/v1/sar/generate', data),
    getNarrative:  (id) => apiClient.get(`/api/v1/sar/${id}`),
    getAuditTrail: (id) => apiClient.get(`/api/v1/sar/${id}/audit`),
    approveSAR:    (id) => apiClient.post(`/api/v1/sar/${id}/approve`),
    getStats:      () => apiClient.get('/api/v1/sar/stats/overview'),
    getRecentCases:() => apiClient.get('/api/v1/sar/'),
}
```

### How React Calls These

In a page component (e.g., `Dashboard.tsx`):

```tsx
const [stats, setStats] = useState<DashboardStats | null>(null)

useEffect(() => {
    const fetchStats = async () => {
        const data = await api.getStats()
        setStats(data)
    }
    fetchStats()
}, [])
```

React's `useEffect` hook runs on component mount, calls the API, and stores the result in state. The component then re-renders with the data.

---

## 15. Docker & Deployment

### Docker Compose Architecture

```yaml
# docker-compose.yml
services:
  backend:
    build: ./backend    # Uses backend/Dockerfile
    ports: 8000:8000    # Expose API
    env_file: .env      # Load environment variables
    environment:
      OLLAMA_HOST: http://host.docker.internal:11434  # Connect to host's Ollama

  frontend:
    build: ./frontend   # Uses frontend/Dockerfile (multi-stage)
    ports: 3000:80      # Nginx serves on port 3000
    depends_on: backend # Start after backend
```

### Backend Dockerfile

```dockerfile
FROM python:3.11-slim       # Small Python image
RUN apt-get install gcc libpq-dev  # PostgreSQL client library
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Frontend Dockerfile (Multi-Stage Build)

```dockerfile
# Stage 1: Build
FROM node:20-alpine AS builder
RUN npm install
RUN npm run build         # Creates optimized production bundle

# Stage 2: Serve
FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
CMD ["nginx", "-g", "daemon off;"]
```

The multi-stage build means the final image only contains the compiled HTML/JS/CSS (a few MB), not the node_modules (hundreds of MB).

---

## 16. What I Built & In What Order

Here's the chronological development story:

| # | Phase | What Was Done |
|---|---|---|
| 1 | **Project Setup** | Created `.gitignore`, repo structure, and initial README |
| 2 | **Backend Core** | Set up FastAPI app, Pydantic Settings, SQLAlchemy database connection |
| 3 | **Database Models** | Created 5 SQLAlchemy ORM models (Customer, Transaction, SARCase, SARNarrative, AuditTrail) |
| 4 | **API Schemas** | Wrote Pydantic request/response schemas for type-safe API |
| 5 | **API Routes** | Implemented all FastAPI endpoints (health, generate, get, audit, approve, stats) |
| 6 | **ML Pattern Detection** | Built 4 detection algorithms (velocity, volume, structuring, network) + typology matching + risk scoring |
| 7 | **RAG Service** | Integrated ChromaDB + sentence-transformers for SAR template retrieval |
| 8 | **LLM Integration** | Connected to Ollama via HTTP, built grounded prompt templates with anti-hallucination instructions |
| 9 | **Audit Logger** | Implemented SHA-256 hash-chained audit trail with sentence-level attribution |
| 10 | **Validator** | Built narrative validator to cross-check amounts/dates against source data |
| 11 | **LangGraph Workflow** | Orchestrated all 6 steps into a single pipeline with state management |
| 12 | **Data Generation** | Created synthetic data script for 5 customers + 120+ transactions + 5 SAR cases |
| 13 | **Frontend Setup** | Initialized Vite + React + TypeScript + TailwindCSS project |
| 14 | **Frontend Pages** | Built Dashboard (stats + cases), GenerateSAR (6-step progress), SAREditor (audit panel) |
| 15 | **Docker** | Wrote Dockerfiles + docker-compose.yml for containerized deployment |
| 16 | **Debugging** | Fixed Supabase connectivity issues, implemented SQLite fallback, refactored models for cross-DB compatibility |
| 17 | **Documentation** | Comprehensive README + this brief document |

---

## 17. Essential Topics to Learn

If you want to build projects like LuminaSAR, here are the **specific subtopics** you should learn (not the entire framework â€” just what matters):

### ğŸ Python / Backend

| Topic | What to Learn | Why It Matters |
|---|---|---|
| **FastAPI Basics** | Route decorators (`@app.get`, `@app.post`), path parameters, query parameters | You define your API endpoints with these |
| **FastAPI Dependency Injection** | `Depends()`, how `yield` works in dependencies | Clean database session management |
| **Pydantic Models** | `BaseModel`, field types, validators, `BaseSettings` | Request validation, config management |
| **Async/Await** | `async def`, `await`, `httpx.AsyncClient` | LLM calls are slow; async prevents blocking |
| **SQLAlchemy ORM** | `Column`, `Base`, `Session`, `query().filter()`, `create_engine` | All database operations go through this |
| **Environment Variables** | `python-dotenv`, `.env` files, `os.getenv()` | Never hardcode secrets |

### ğŸ¤– AI / ML

| Topic | What to Learn | Why It Matters |
|---|---|---|
| **Embeddings** | What are vector embeddings, cosine similarity, `sentence-transformers` | The foundation of RAG |
| **Vector Databases** | ChromaDB (or Pinecone, Weaviate), collections, upsert, query | Storing and searching embeddings |
| **RAG Pattern** | Load documents â†’ embed â†’ store â†’ query â†’ retrieve â†’ inject into prompt | The core pattern for grounded AI |
| **Prompt Engineering** | System prompts, few-shot examples, grounding, temperature | How you control LLM output quality |
| **Ollama API** | `/api/generate`, model parameters, streaming vs non-streaming | Local LLM integration |
| **LangGraph** | `StateGraph`, nodes, edges, `TypedDict` state | Multi-step AI workflows |

### âš›ï¸ React / Frontend

| Topic | What to Learn | Why It Matters |
|---|---|---|
| **React Hooks** | `useState`, `useEffect`, `useCallback` | State management and side effects |
| **React Router** | `BrowserRouter`, `Route`, `useNavigate`, `useParams` | Page navigation |
| **Axios / Fetch** | HTTP requests, `async/await`, error handling | Calling backend APIs |
| **TypeScript Interfaces** | `interface`, generic types, `Record<string, any>` | Type-safe API responses |
| **TailwindCSS** | Utility classes, responsive design (`md:`, `lg:`), custom themes | Rapid UI styling |
| **Component Patterns** | Props, state lifting, conditional rendering | Building reusable UI pieces |

### ğŸ—ï¸ Architecture / DevOps

| Topic | What to Learn | Why It Matters |
|---|---|---|
| **REST API Design** | HTTP methods (GET/POST), status codes, JSON, CORS | Frontend-backend communication |
| **Docker Basics** | `Dockerfile`, `docker-compose.yml`, images, containers, volumes | Reproducible deployments |
| **Database Design** | Primary keys, foreign keys, 1-to-many relationships, JSON columns | Schema planning |
| **Git** | Branching, staging, committed, meaningful commit messages | Version control |

### ğŸ” Security Concepts

| Topic | What to Learn | Why It Matters |
|---|---|---|
| **Hashing** | SHA-256, `hashlib`, hash chains | Audit trail integrity |
| **CORS** | What it is, why browsers block cross-origin, how to configure | Frontend-backend on different ports |
| **JWT** | JSON Web Tokens, signing, verification | User authentication |
| **Environment Security** | `.env` in `.gitignore`, secrets management | Never commit passwords |

---

## 18. Similar Project Ideas

Here are project ideas that use the **same tech stack and patterns** as LuminaSAR:

### ğŸ”¹ Beginner-Friendly (Same patterns, simpler domain)

| Project | Tech Used | Description |
|---|---|---|
| **AI Resume Screener** | FastAPI + Ollama + React | Upload resumes, LLM extracts skills/experience, compare against job descriptions using RAG |
| **Smart Meeting Summarizer** | FastAPI + Ollama + ChromaDB | Upload meeting transcripts, generate summaries with action items, search past meetings via RAG |
| **AI Study Notes Generator** | FastAPI + Ollama + React | Upload textbook chapters, LLM generates study notes, flashcards, and quiz questions with source attribution |

### ğŸ”¹ Intermediate (More complex pipeline)

| Project | Tech Used | Description |
|---|---|---|
| **Medical Report Analyzer** | FastAPI + LangGraph + Ollama + RAG | Upload lab reports, detect anomalies, generate patient-friendly summaries with source citations |
| **Legal Contract Reviewer** | FastAPI + RAG + Ollama | Upload contracts, compare against regulatory templates via RAG, highlight risky clauses with audit trail |
| **AI Code Reviewer** | FastAPI + Ollama + ChromaDB | Submit code, retrieve coding standards via RAG, generate code review with line-by-line annotations |
| **Customer Support Bot** | FastAPI + RAG + React | Build a chatbot that retrieves answers from company docs (RAG), logs every response with source attribution |

### ğŸ”¹ Advanced (Full production-grade)

| Project | Tech Used | Description |
|---|---|---|
| **Financial Fraud Detection Dashboard** | FastAPI + ML + LangGraph + React | Real-time transaction monitoring, ML anomaly detection, automated alert generation with audit trail |
| **AI-Powered Research Assistant** | LangGraph + RAG + Ollama | Multi-step research workflow: search â†’ extract â†’ synthesize â†’ cite, with hash-chained provenance |
| **Regulatory Compliance Copilot** | FastAPI + RAG + Ollama + React | Upload company policies, compare against regulations (GDPR, HIPAA), generate compliance reports |
| **Explainable AI Diagnostic Tool** | FastAPI + ML + LangGraph | Run ML predictions and generate human-readable explanations with sentence-level attribution to input features |

### ğŸ”¹ The Learning Progression

```
[ Start Here ]
    â”‚
    â–¼
Build a simple API with FastAPI
    â”‚  (Learn: routes, Pydantic, SQLAlchemy)
    â–¼
Add a React frontend
    â”‚  (Learn: axios, useState, useEffect)
    â–¼
Connect to Ollama for text generation
    â”‚  (Learn: HTTP API, prompt engineering)
    â–¼
Add RAG with ChromaDB
    â”‚  (Learn: embeddings, vector search)
    â–¼
Build a multi-step pipeline with LangGraph
    â”‚  (Learn: state management, async workflows)
    â–¼
Add audit trail and validation
    â”‚  (Learn: hashing, data integrity)
    â–¼
Dockerize everything
    â”‚  (Learn: containers, compose, deployment)
    â–¼
[ You're now at LuminaSAR level! ]
```

---

<p align="center">
  <strong>LuminaSAR</strong> â€” Because compliance shouldn't be a black box. ğŸ”†
</p>
