# ðŸ”† LuminaSAR â€” The Glass Box AI

> *SAR Narrative Generator with Explainable AI Audit Trail*
>
> **Barclays Hack-O-Hire 2026**

<p align="center">
  <strong>Where Every Decision is Transparent</strong>
</p>

---

## ðŸš€ What is LuminaSAR?

LuminaSAR is an AI-powered **Suspicious Activity Report (SAR) narrative generator** that reduces manual SAR writing from **5-6 hours â†’ ~30 seconds** while maintaining **complete explainability** through a sentence-level audit trail.

### The Innovation: Sentence-Level Data Attribution

Unlike black-box AI systems, LuminaSAR provides a **Glass Box** approach:
- Every sentence in the generated narrative links back to the exact source data
- A **hash-chained audit trail** provides tamper-evident logging of every AI decision
- Analysts can click any sentence to see which transactions, patterns, and templates informed it

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LuminaSAR Architecture                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend   â”‚         Backend API             â”‚   Data     â”‚
â”‚  React/TS   â”‚         FastAPI                 â”‚  Layer     â”‚
â”‚  Tailwind   â”‚                                  â”‚            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Dashboardâ”‚ â”‚  â”‚   LangGraph Workflow     â”‚   â”‚ â”‚Supabaseâ”‚ â”‚
â”‚  â”‚Generate â”‚â”€â”¼â”€â”€â”‚ 1. Fetch Data           â”‚â”€â”€â”€â”¼â”€â”‚PostgreSâ”‚ â”‚
â”‚  â”‚Editor   â”‚ â”‚  â”‚ 2. Analyze Patterns (ML)â”‚   â”‚ â”‚  QL    â”‚ â”‚
â”‚  â”‚AuditViewâ”‚ â”‚  â”‚ 3. RAG Template Lookup  â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ 4. LLM Generation       â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚             â”‚  â”‚ 5. Validate (anti-halluc)â”‚   â”‚ â”‚ SQLite â”‚ â”‚
â”‚             â”‚  â”‚ 6. Save + Audit Trail   â”‚   â”‚ â”‚Fallbackâ”‚ â”‚
â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚             â”‚  â”‚ Ollama     â”‚ â”‚  Audit   â”‚   â”‚ â”‚ChromaDBâ”‚ â”‚
â”‚             â”‚  â”‚ llama3.2   â”‚ â”‚  Logger  â”‚   â”‚ â”‚  RAG   â”‚ â”‚
â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ðŸ¤– **AI Narrative Generation** | Ollama llama3.2 generates regulatory-compliant SAR narratives |
| ðŸ” **ML Pattern Detection** | Velocity, volume, structuring, network analysis algorithms |
| ðŸ“š **RAG Templates** | ChromaDB + sentence-transformers for context-aware generation |
| ðŸ”— **Hash-Chained Audit** | SHA-256 hash chain for tamper-evident audit trail |
| ðŸ“ **Sentence Attribution** | Click any sentence â†’ see exact source data |
| ðŸ·ï¸ **Typology Matching** | Structuring, layering, smurfing, integration, round-tripping |
| ðŸ“Š **Risk Scoring** | 0-10 risk score from multi-factor analysis |
| ðŸ”’ **100% Local** | No data leaves your network â€” Ollama runs locally |
| ðŸ”„ **Auto DB Fallback** | Automatic SQLite fallback if Supabase is unreachable |

---

## ðŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| **Frontend** | React 18, TypeScript, TailwindCSS, React Query, Zustand, Framer Motion |
| **Backend** | FastAPI, SQLAlchemy, Pydantic |
| **AI/ML** | Ollama (llama3.2), LangGraph, ChromaDB, sentence-transformers |
| **Database** | Supabase PostgreSQL (primary) / SQLite (automatic fallback) |
| **Analysis** | Pandas, NumPy, Scikit-learn, NetworkX |
| **Infra** | Docker, Docker Compose, Nginx |

---

## ðŸ“‹ Prerequisites

Before cloning the repo, make sure you have the following installed on your machine:

| Requirement | Version | Download Link |
|-------------|---------|---------------|
| **Python** | 3.11+ | [python.org](https://www.python.org/downloads/) |
| **Node.js** | 18+ | [nodejs.org](https://nodejs.org/) |
| **Ollama** | Latest | [ollama.com](https://ollama.com/download) |
| **Git** | Latest | [git-scm.com](https://git-scm.com/) |
| **Docker Desktop** | Latest *(optional)* | [docker.com](https://www.docker.com/products/docker-desktop/) |

> **Note:** Docker is only needed if you want containerized deployment. The project runs perfectly fine without it using the local development setup.

---

## âš¡ Quick Start (Local Development)

### Step 1: Clone the Repository

```bash
git clone https://github.com/yash-7575/LuminaSAR.git
cd LuminaSAR
```

---

### Step 2: Backend Setup

```bash
cd backend

# Create and activate a virtual environment
python -m venv venv

# Windows
venv\Scripts\activate

# macOS / Linux
source venv/bin/activate

# Install all Python dependencies
pip install -r requirements.txt
```

> **âš ï¸ Troubleshooting:** If `pip install` hangs or fails with dependency conflicts, try:
> ```bash
> pip install --no-cache-dir -r requirements.txt
> ```
> Or install core packages first:
> ```bash
> pip install fastapi uvicorn sqlalchemy pydantic pydantic-settings python-dotenv
> pip install langchain langchain-community langgraph chromadb sentence-transformers ollama
> pip install pandas numpy scikit-learn networkx httpx
> ```

---

### Step 3: Configure Environment Variables

Create or edit the file `backend/.env`:

```env
# Supabase Configuration (Optional â€” the app auto-falls-back to SQLite)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your_anon_key_here
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here
DATABASE_URL=postgresql://postgres:your_password@db.your-project.supabase.co:5432/postgres

# JWT
JWT_SECRET_KEY=change_me_to_64_char_hex_string

# Ollama (REQUIRED â€” this is the local LLM)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# ChromaDB
CHROMA_PERSIST_DIR=./chroma_db
```

> **ðŸ’¡ Important:** If you don't have Supabase credentials, that's perfectly fine! The backend will automatically fall back to a local **SQLite database** (`luminasar.db`) and work out of the box. Supabase is only needed for cloud PostgreSQL.

---

### Step 4: Install and Start Ollama

Ollama is the **local LLM runtime** that powers the narrative generation. It runs entirely on your machine â€” no API keys, no cloud, no data leakage.

```bash
# Install Ollama from https://ollama.com/download

# Pull the required model (~2GB download)
ollama pull llama3.2:latest

# Start the Ollama server (it runs on port 11434)
ollama serve
```

> **âœ… Verify Ollama is running:**
> ```bash
> curl http://localhost:11434/api/tags
> ```
> You should see a JSON response listing `llama3.2:latest`.

---

### Step 5: Seed the Database with Test Data

```bash
cd backend
python -m scripts.generate_data
```

This will:
- Generate **5 synthetic customers** with Indian bank accounts
- Create **~120+ transactions** with suspicious patterns (structuring, layering, smurfing, etc.)
- Create **5 SAR cases** ready for narrative generation
- Print **Case IDs** you can use for testing

Example output:
```
ðŸ—ï¸ LuminaSAR â€” Generating Synthetic Data
==================================================

ðŸ‘¤ Customer: Sanjay Verma (AXIS206078254)
   ðŸ’³ 21 transactions (structuring)
   ðŸ“‹ Case: b920a3c4...

...

ðŸ“‹ Case IDs for testing:
   b920a3c4-dccc-44ce-9ebb-9866edcec4dd
   8e23cac6-6d07-4d92-bc7b-79092dbfce65
   ...
```

> **ðŸ“ Save one of these Case IDs** â€” you'll need it to generate a SAR narrative.

---

### Step 6: Start the Backend Server

```bash
cd backend
uvicorn app.main:app --reload --port 8000
```

The API will be available at:
- **API Root:** http://localhost:8000
- **Swagger Docs:** http://localhost:8000/docs
- **Health Check:** http://localhost:8000/health

---

### Step 7: Start the Frontend

```bash
cd frontend

# Install Node.js dependencies
npm install

# Start the development server
npm run dev
```

The frontend will be available at: **http://localhost:5173**

---

### Step 8: Generate Your First SAR! ðŸš€

1. Open **http://localhost:5173** in your browser
2. You'll see the **Dashboard** with pending cases and statistics
3. Click **"New SAR"** to go to the generation page
4. Enter a **Case ID** from Step 5 (e.g., `b920a3c4-dccc-44ce-9ebb-9866edcec4dd`)
5. Click **"Generate SAR"** and watch the 6-step pipeline in action
6. Review the generated narrative in the **SAR Editor** with click-to-audit functionality

---

## ðŸ³ Docker Deployment (Alternative)

If you prefer containerized deployment:

```bash
# Make sure Ollama is running on the host machine first!
ollama serve

# Build and start all services
docker-compose up --build
```

This starts:
| Service | URL | Description |
|---------|-----|-------------|
| **Backend** | http://localhost:8000 | FastAPI server |
| **Frontend** | http://localhost:3000 | React app (via Nginx) |

> **Note:** When running in Docker, the backend connects to Ollama on the host via `host.docker.internal:11434`. Make sure Ollama is running before starting Docker.

---

## ðŸ“‹ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Service health check |
| `GET` | `/docs` | Interactive Swagger API documentation |
| `POST` | `/api/v1/sar/generate` | Generate a SAR narrative |
| `GET` | `/api/v1/sar/{narrative_id}` | Get narrative details |
| `GET` | `/api/v1/sar/{narrative_id}/audit` | Get complete audit trail |
| `POST` | `/api/v1/sar/{narrative_id}/approve` | Approve SAR for filing |
| `GET` | `/api/v1/sar/` | List recent SAR cases |
| `GET` | `/api/v1/sar/stats/overview` | Dashboard statistics |

### Example: Generate a SAR

```bash
curl -X POST http://localhost:8000/api/v1/sar/generate \
  -H "Content-Type: application/json" \
  -d '{"case_id": "YOUR_CASE_ID_HERE"}'
```

### Example: Get Dashboard Stats

```bash
curl http://localhost:8000/api/v1/sar/stats/overview
```

Response:
```json
{
  "total_sars": 1,
  "pending_cases": 4,
  "avg_generation_time": 34.0,
  "total_customers": 5,
  "high_risk_cases": 0,
  "cost_savings_lakhs": 0.1
}
```

---

## ðŸ” The 6-Step AI Pipeline

Each step is logged to the audit trail with data sources, reasoning, confidence scores, and SHA-256 hash links.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. FETCH    â”‚â”€â”€â”€â–¶â”‚  2. ANALYZE      â”‚â”€â”€â”€â–¶â”‚  3. RAG RETRIEVE  â”‚
â”‚  Customer &  â”‚    â”‚  ML Pattern      â”‚    â”‚  Template lookup  â”‚
â”‚  Transaction â”‚    â”‚  Detection       â”‚    â”‚  via ChromaDB     â”‚
â”‚  Data        â”‚    â”‚  (4 algorithms)  â”‚    â”‚  sentence-embed   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. SAVE     â”‚â—€â”€â”€â”€â”‚  5. VALIDATE     â”‚â—€â”€â”€â”€â”‚  4. GENERATE      â”‚
â”‚  Narrative + â”‚    â”‚  Anti-halluc     â”‚    â”‚  LLM Narrative    â”‚
â”‚  Audit Trail â”‚    â”‚  Cross-check     â”‚    â”‚  via Ollama       â”‚
â”‚  to Database â”‚    â”‚  Against Source   â”‚    â”‚  (grounded)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step Details

| Step | Service | What It Does |
|------|---------|-------------|
| **1. Fetch Data** | `langgraph_workflow.py` | Queries customer KYC and transaction records from the database |
| **2. Analyze Patterns** | `pattern_detector.py` | Runs 4 ML algorithms: velocity analysis, volume analysis, structuring detection (â‚¹50K threshold), network graph analysis (NetworkX) |
| **3. Retrieve Templates** | `rag_service.py` | Searches ChromaDB vector store for relevant SAR templates using `all-MiniLM-L6-v2` embeddings |
| **4. Generate Narrative** | `llm_service.py` | Constructs a grounded prompt with all data + templates, sends to Ollama `llama3.2` with anti-hallucination instructions |
| **5. Validate** | `validator.py` | Cross-references generated text against source data for factual accuracy, checks for hallucinated amounts/dates |
| **6. Save Results** | `audit_logger.py` | Persists narrative + creates sentence-level attribution + builds SHA-256 hash chain |

---

## ðŸ“Š Database Schema

| Table | Purpose | Key Columns |
|-------|---------|-------------|
| `customers` | Customer KYC data | `customer_id`, `name`, `account_number`, `occupation`, `stated_income`, `customer_since` |
| `transactions` | Transaction records | `transaction_id`, `customer_id`, `amount`, `date`, `source_account`, `destination_account`, `transaction_type` |
| `sar_cases` | SAR case metadata | `case_id`, `customer_id`, `status`, `risk_score`, `typologies` (JSON) |
| `sar_narratives` | Generated narrative text | `narrative_id`, `case_id`, `narrative_text`, `generated_at`, `generation_time_seconds` |
| `audit_trail` | Hash-chained audit steps | `audit_id`, `narrative_id`, `step_name`, `data_sources` (JSON), `reasoning` (JSON), `confidence_scores` (JSON), `previous_hash`, `current_hash` |

> **Database Flexibility:** The models use standard SQLAlchemy types (`String`, `JSON`, `DateTime`) instead of Postgres-specific types, allowing seamless switching between Supabase PostgreSQL and local SQLite.

---

## ðŸ“ Project Structure

```
LuminaSAR/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py              # Pydantic Settings from .env
â”‚   â”‚   â”œâ”€â”€ database.py            # SQLAlchemy engine + SQLite fallback
â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI app with CORS
â”‚   â”‚   â”œâ”€â”€ models/                # SQLAlchemy ORM models
â”‚   â”‚   â”‚   â”œâ”€â”€ customer.py        # Customer KYC model
â”‚   â”‚   â”‚   â”œâ”€â”€ transaction.py     # Transaction model
â”‚   â”‚   â”‚   â”œâ”€â”€ sar_case.py        # SAR case model
â”‚   â”‚   â”‚   â”œâ”€â”€ sar_narrative.py   # Generated narrative model
â”‚   â”‚   â”‚   â””â”€â”€ audit_trail.py     # Hash-chained audit model
â”‚   â”‚   â”œâ”€â”€ routes/                # FastAPI endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py          # GET /health
â”‚   â”‚   â”‚   â””â”€â”€ sar.py             # All SAR CRUD + generation routes
â”‚   â”‚   â”œâ”€â”€ schemas/               # Pydantic request/response models
â”‚   â”‚   â”‚   â”œâ”€â”€ request.py         # GenerateSARRequest, ApproveSARRequest
â”‚   â”‚   â”‚   â””â”€â”€ response.py        # SARResponse, AuditTrailResponse, etc.
â”‚   â”‚   â”œâ”€â”€ services/              # Core AI/ML services
â”‚   â”‚   â”‚   â”œâ”€â”€ pattern_detector.py    # ML pattern detection algorithms
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py         # ChromaDB + embeddings RAG
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py         # Ollama LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ audit_logger.py        # Hash-chained audit trail
â”‚   â”‚   â”‚   â”œâ”€â”€ validator.py           # Anti-hallucination validation
â”‚   â”‚   â”‚   â””â”€â”€ langgraph_workflow.py  # 6-step pipeline orchestrator
â”‚   â”‚   â””â”€â”€ utils/                 # Utilities
â”‚   â”‚       â”œâ”€â”€ hash.py            # SHA-256 hash functions
â”‚   â”‚       â””â”€â”€ prompts.py         # SAR generation prompt templates
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ templates/             # SAR reference templates
â”‚   â”‚       â”œâ”€â”€ sar_structuring.txt
â”‚   â”‚       â”œâ”€â”€ sar_layering.txt
â”‚   â”‚       â”œâ”€â”€ sar_smurfing.txt
â”‚   â”‚       â””â”€â”€ sar_integration.txt
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ generate_data.py       # Synthetic data generation
â”‚   â”œâ”€â”€ luminasar.db               # Local SQLite fallback (auto-generated)
â”‚   â”œâ”€â”€ chroma_db/                 # ChromaDB vector store (auto-generated)
â”‚   â”œâ”€â”€ .env                       # Environment variables
â”‚   â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚   â””â”€â”€ Dockerfile                 # Backend container
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx                # React Router setup
â”‚   â”‚   â”œâ”€â”€ main.tsx               # App entry point
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ Navbar.tsx         # Navigation bar
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx      # Stats widgets, case table
â”‚   â”‚   â”‚   â”œâ”€â”€ GenerateSAR.tsx    # 6-step animated generation UI
â”‚   â”‚   â”‚   â””â”€â”€ SAREditor.tsx      # Sentence-level audit trail viewer
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts             # Axios API client
â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”‚       â””â”€â”€ globals.css        # Glassmorphism, glow effects
â”‚   â”œâ”€â”€ index.html                 # HTML entry point
â”‚   â”œâ”€â”€ tailwind.config.js         # TailwindCSS configuration
â”‚   â”œâ”€â”€ vite.config.ts             # Vite bundler config
â”‚   â”œâ”€â”€ tsconfig.json              # TypeScript config
â”‚   â”œâ”€â”€ package.json               # Node.js dependencies
â”‚   â”œâ”€â”€ nginx.conf                 # Production Nginx config
â”‚   â””â”€â”€ Dockerfile                 # Frontend container
â”œâ”€â”€ docker-compose.yml             # Multi-service orchestration
â””â”€â”€ README.md                      # This file
```

---

## ðŸ”§ Environment Variables Reference

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `SUPABASE_URL` | No | â€” | Supabase project URL |
| `SUPABASE_ANON_KEY` | No | â€” | Supabase anonymous key |
| `SUPABASE_SERVICE_ROLE_KEY` | No | â€” | Supabase service role key (for data seeding) |
| `DATABASE_URL` | No | SQLite fallback | PostgreSQL connection string |
| `JWT_SECRET_KEY` | No | `change_me...` | JWT signing secret |
| `OLLAMA_HOST` | **Yes** | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_MODEL` | **Yes** | `llama3.2:latest` | LLM model to use |
| `CHROMA_PERSIST_DIR` | No | `./chroma_db` | ChromaDB storage directory |

---

## â“ Frequently Asked Questions

### "I don't have Supabase â€” will it work?"

**Yes!** The backend automatically detects if Supabase is unreachable and falls back to a local **SQLite** database (`luminasar.db`). All features work identically.

### "Which Ollama model should I use?"

We recommend `llama3.2:latest` (default). It's ~2GB and runs on most modern machines. For faster generation on lower-end hardware, try `llama3.2:1b`.

```bash
# Default (recommended)
ollama pull llama3.2:latest

# Lighter alternative
ollama pull llama3.2:1b
```

### "How long does SAR generation take?"

Typically **25-40 seconds** depending on your hardware. The bottleneck is the LLM generation step (Step 4). GPU-accelerated Ollama will be significantly faster.

### "I'm getting `ModuleNotFoundError`"

Make sure you:
1. Activated your virtual environment (`venv\Scripts\activate` on Windows)
2. Installed all dependencies (`pip install -r requirements.txt`)
3. Are running commands from the `backend/` directory

### "The frontend can't connect to the backend"

The frontend expects the backend at `http://localhost:8000`. Make sure:
1. The backend is running on port 8000
2. CORS is configured (it is by default for `localhost:5173`)

### "`pip install` is stuck on backtracking"

This can happen with complex dependency chains. Try:
```bash
pip install --no-cache-dir -r requirements.txt
```

---

## ðŸŽ¯ Hackathon Problem Statement

> **Barclays Hack-O-Hire 2026**: Build a production-grade GenAI tool that automates SAR narrative filing with full explainability.

### Our Differentiators

| Feature | Details |
|---------|---------|
| âœ… **Sentence-Level Attribution** | Not just "what" the AI generated, but "why" for each sentence |
| âœ… **Hash-Chained Audit Trail** | SHA-256 tamper-evident, regulatory-compliant logging |
| âœ… **100% Local LLM** | No sensitive data leaves the bank's network |
| âœ… **Indian Regulatory Compliance** | PMLA, FIU-IND, â‚¹50K CTR threshold detection |
| âœ… **Multi-Typology Detection** | ML-driven (not rule-based) â€” structuring, layering, smurfing, integration |
| âœ… **Anti-Hallucination Validation** | Cross-references every generated fact against source data |
| âœ… **Production-Ready** | Docker deployment, auto-fallback database, health checks |

---

## ðŸ§ª Running Tests

```bash
cd backend
python -m pytest tests/ -v
```

---

## ðŸ“œ License

This project was built for Barclays Hack-O-Hire 2026.


---

<p align="center">
  <strong>LuminaSAR</strong> â€” Because compliance shouldn't be a black box. ðŸ”†
</p>
